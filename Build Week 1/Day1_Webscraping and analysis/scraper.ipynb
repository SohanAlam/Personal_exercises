{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url scraper - after running imports, run cell below, it scraps all links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_'\n",
    "driver = (\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(driver)\n",
    "urls = []\n",
    "driver.get(main_url)\n",
    "\n",
    "for i in range(1,38):\n",
    "    if i ==2:\n",
    "        sleep(3)\n",
    "        x_button = driver.find_element_by_xpath('/html/body/div[3]/div/div/div[1]/button/img')\n",
    "        x_button.click()\n",
    "    book_table = driver.find_element_by_xpath('//table[@class=\"tableList js-dataTooltip\"]')\n",
    "    books = book_table.find_elements_by_tag_name('tr')\n",
    "\n",
    "    for book in books:\n",
    "        info = book.find_elements_by_tag_name('td')[2]\n",
    "        b_title = info.find_element_by_class_name('bookTitle')\n",
    "        urls.append(b_title.get_attribute('href'))\n",
    "    next_button = driver.find_element_by_xpath('//a[@class=\"next_page\"]')\n",
    "    try:\n",
    "        next_button.click()\n",
    "    except:\n",
    "        print(print('it was last one'))\n",
    "    \n",
    "    sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url_path):\n",
    "    titles = []\n",
    "    avg_ratings = []\n",
    "    authors = []\n",
    "    num_ratings = []\n",
    "    num_revs = []\n",
    "    num_pages = []\n",
    "    publ_years = []\n",
    "    is_series = []\n",
    "    genres = []\n",
    "    awards = []\n",
    "    places = []\n",
    "    pub_years_1 = []\n",
    "    book_urls = []\n",
    "    ################ starting point ############################\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    counter =0\n",
    "    pl_check = 0\n",
    "    place = ''\n",
    "    for url in url_path:\n",
    "        counter +=1\n",
    "        if counter %25==0:\n",
    "            print('{} books are scraped, 2 seconds of rest'.format(counter))\n",
    "            if counter%100==0:\n",
    "                sleep(3)\n",
    "            sleep(2)\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            box = driver.find_element_by_id('metacol')\n",
    "            ######## extracting info about title ##############\n",
    "            title = box.find_element_by_id('bookTitle').text\n",
    "            titles.append(title)\n",
    "            ######## extracting info about average rating ##############\n",
    "            author_box = box.find_element_by_id('bookAuthors')\n",
    "            author = author_box.find_elements_by_tag_name('a')[0].text\n",
    "            authors.append(author)\n",
    "            ######## extracting info about average rating ##############\n",
    "            rat_rev_box = box.find_element_by_id('bookMeta')\n",
    "            avg_rating = rat_rev_box.find_elements_by_tag_name('span')[6].text\n",
    "            avg_ratings.append(float(avg_rating))\n",
    "            ######## extracting info about number of ratings ##############\n",
    "            num_rating = rat_rev_box.find_elements_by_tag_name('a')[1].text.split()[0].replace(',','')\n",
    "            num_ratings.append(int(num_rating))\n",
    "            ######## extracting info about number of reviews ##############\n",
    "            num_rev = rat_rev_box.find_elements_by_tag_name('a')[2].text.split()[0].replace(',','')\n",
    "            num_revs.append(int(num_rev))\n",
    "            ########## finds element containing pages, pub year\n",
    "            details = box.find_element_by_id('details')\n",
    "            ######## is it aprt of book series ################\n",
    "            series = box.find_element_by_id('bookSeries')\n",
    "            if len(series.text)>0:\n",
    "                is_series.append(1)\n",
    "            else:\n",
    "                is_series.append(0)\n",
    "            ######## extracting info about pages ##############\n",
    "            page_check = 0\n",
    "            try:\n",
    "                pages = details.find_elements_by_tag_name('div')[0].text.split()\n",
    "                for i in pages:\n",
    "                    if i.isnumeric():\n",
    "                        num_pages.append(int(i))\n",
    "                        page_check=1\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            if not page_check:\n",
    "                num_pages.append(None)\n",
    "            ####### extracting info about pub year ##############\n",
    "            year_check = 0\n",
    "            try:\n",
    "                years_info = details.find_elements_by_tag_name('div')[1]\n",
    "                pub_years = years_info.text.split()\n",
    "                for year in pub_years:\n",
    "                    if len(year)==4 and year.isnumeric():\n",
    "                        publ_years.append(int(year))\n",
    "                        year_check = 1\n",
    "            except:\n",
    "                pass\n",
    "            if not year_check:\n",
    "                publ_years.append(None)\n",
    "            ####### extracting info about pub year first ##############\n",
    "            year_check_1 = 0\n",
    "            try:\n",
    "                pub_years_first = years_info.find_elements_by_class_name('greyText')[0].text.replace('(','').replace(')','').split()\n",
    "                for yr in pub_years_first:\n",
    "                    if len(yr)==4 and yr.isnumeric():\n",
    "                        pub_years_1.append(int(yr))\n",
    "                        year_check_1 = 1\n",
    "            except:\n",
    "                pass\n",
    "            if not year_check_1:\n",
    "                pub_years_1.append(None)\n",
    "            ######## extracting info about genre ##############\n",
    "            c = 0\n",
    "            try:\n",
    "                genres_box = driver.find_element_by_xpath('//div[@class=\"stacked\"]').find_element_by_xpath('.//div[@class=\"bigBoxContent containerWithHeaderContent\"]').find_elements_by_class_name('elementList')[:3]\n",
    "                genre_complete=''\n",
    "                for genre in genres_box:\n",
    "                    genre_complete+=',{}'.format(genre.find_elements_by_tag_name('div')[0].text)\n",
    "                genres.append(genre_complete[1:])\n",
    "                c=1\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            if not c:\n",
    "                genres.append(None)\n",
    "            ######## extracting info about awards ##############\n",
    "            more_info_button = details.find_element_by_id('bookDataBoxShow')\n",
    "            try:\n",
    "                more_info_button.click()\n",
    "            except:\n",
    "                pass\n",
    "            award_places = details.find_elements_by_tag_name('div')[2].find_elements_by_tag_name('div')[0]\n",
    "            award_box = award_places.find_elements_by_class_name('clearFloats')\n",
    "            settings = award_places.find_elements_by_class_name('infoBoxRowItem')\n",
    "            for box in award_box:\n",
    "                check = 0\n",
    "                try:\n",
    "                    award = box.find_element_by_xpath('.//div[@itemprop=\"awards\"]')\n",
    "                    awards.append(award.text)\n",
    "                    check = 1\n",
    "                except:\n",
    "                    pass\n",
    "            if not check:\n",
    "                awards.append(None)\n",
    "            ####### extracting info about places ##############\n",
    "            for setting in settings:\n",
    "                plcs = setting.find_elements_by_tag_name('a')\n",
    "                for plc in plcs:\n",
    "                    linking = plc.get_attribute('href')\n",
    "                    if '/places/' in linking:\n",
    "                        place += ',{}'.format(plc.text)\n",
    "                        pl_check =1\n",
    "\n",
    "            if not pl_check:\n",
    "                places.append(None)\n",
    "            else:\n",
    "                places.append(place[1:])\n",
    "                place=''\n",
    "            pl_check = 0\n",
    "            book_urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    data = {\n",
    "    \"title\":titles,\n",
    "    \"author\":authors,\n",
    "    \"num_reviews\":num_revs,\n",
    "    \"num_ratings\":num_ratings,\n",
    "    \"avg_rating\":avg_ratings,\n",
    "    \"num_pages\":num_pages,\n",
    "    \"publish_year\":publ_years,\n",
    "    \"first_published\":pub_years_1,\n",
    "    \"series\":is_series,\n",
    "    \"genres\":genres,\n",
    "    \"awards\":awards,\n",
    "    \"places\":places,\n",
    "    \"url\": book_urls\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Frame creation - as you can see, i provided urls as argument to the scraper function. modifying indexing values, run cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 books are scraped, 2 seconds of rest\n",
      "50 books are scraped, 2 seconds of rest\n",
      "75 books are scraped, 2 seconds of rest\n",
      "100 books are scraped, 2 seconds of rest\n",
      "125 books are scraped, 2 seconds of rest\n",
      "150 books are scraped, 2 seconds of rest\n",
      "175 books are scraped, 2 seconds of rest\n",
      "200 books are scraped, 2 seconds of rest\n",
      "225 books are scraped, 2 seconds of rest\n",
      "250 books are scraped, 2 seconds of rest\n",
      "275 books are scraped, 2 seconds of rest\n",
      "300 books are scraped, 2 seconds of rest\n",
      "325 books are scraped, 2 seconds of rest\n",
      "350 books are scraped, 2 seconds of rest\n",
      "375 books are scraped, 2 seconds of rest\n",
      "400 books are scraped, 2 seconds of rest\n",
      "425 books are scraped, 2 seconds of rest\n",
      "450 books are scraped, 2 seconds of rest\n",
      "475 books are scraped, 2 seconds of rest\n",
      "500 books are scraped, 2 seconds of rest\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7b353f5c4af5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         ]\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = scraper(urls[2500:3000])\n",
    "df = pd.DataFrame(data,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "print(len(data['awards']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after scraping, to save it as csv file, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample_books(740-840).csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83a2f9ca03f55ceb7ba34fffa46dc6dbeecdfd74c901df231d91189a23c4b669"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
