{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threaded-court",
   "metadata": {},
   "source": [
    "Training a neural network, data is often processed in batches. For this reason, it is convenient to load the data in a\n",
    "\n",
    "```\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in data:\n",
    "        train\n",
    "```\n",
    "fashion, where `x_batch, y_batch` contain respectively a batch of samples features and labels.\n",
    "\n",
    "If we have something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-joshua",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:01:00.351865Z",
     "start_time": "2021-06-02T05:01:00.348363Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [1,2,3,4]\n",
    "\n",
    "y = [0,0,1,1]\n",
    "\n",
    "data = X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-treat",
   "metadata": {},
   "source": [
    "we don't get the desired behaviour. In fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-stomach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:01:01.078882Z",
     "start_time": "2021-06-02T05:01:01.068175Z"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in data:\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-immunology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:01:21.510046Z",
     "start_time": "2021-06-02T05:01:21.501972Z"
    }
   },
   "source": [
    "A way to achieve it is by using the `zip` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-final",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:01:38.334914Z",
     "start_time": "2021-06-02T05:01:38.330524Z"
    }
   },
   "outputs": [],
   "source": [
    "data = zip(X,y)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-episode",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:01:48.885847Z",
     "start_time": "2021-06-02T05:01:48.881626Z"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in zip(X, y):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-warrior",
   "metadata": {},
   "source": [
    "Now it's much better! But what if we want to adjust everything in batches? What if we have data that are indexed in a csv file?\n",
    "What if we have images and we want to apply transformation to the data in the exact moment when we load them to process them? (The alternative is to preprocess all of them before loading it: it may cause a lot of storage usage! In fact, if you want to perform data augmentation for images for example, you will create a lot of copies - with slight changes - of the data!)\n",
    "\n",
    "When you have used Pytorch so far you have loaded standard datasets (MNIST, FashionMNIST...) and you have taken advantage of the Pytorch Dataloader already. Let's do the same for a custom dataset, but for that we need to override some of the method given by that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-metabolism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:08:37.583072Z",
     "start_time": "2021-06-02T05:08:37.570456Z"
    }
   },
   "source": [
    "As usual, we import the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-aaron",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:09:06.744694Z",
     "start_time": "2021-06-02T05:09:03.737995Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-electron",
   "metadata": {},
   "source": [
    "We need to inherit from the Dataset class (that we have imported from torch.utils.data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-clerk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:10:12.012741Z",
     "start_time": "2021-06-02T05:10:12.005962Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-peace",
   "metadata": {},
   "source": [
    "Awesome! Now we need to override our constructor and the `__getitem__` method. What is this about? It is the method that allows your CustomDataset to be *indexed* in a `dataset[i]` fashion.\n",
    "\n",
    "In addition, we want to override the `__len__` method as well, that is the method that returns the amount of data samples in the dataset that we are processing.\n",
    "\n",
    "Ok but the shuffling?! *I want to shuffle the data!* Don't worry, the rest of the methods from the `Dataset`class will still be working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-offense",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:30:20.755968Z",
     "start_time": "2021-06-02T05:30:20.736754Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # we want to be index like dataset[index]\n",
    "        # to get the index-th batch\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # to retrieve the total samples by doing len(dataset)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-radical",
   "metadata": {},
   "source": [
    "As you can see, in the constructor I added `csv_file` as argument. The reason is that I want to create a dataloader for a dataset containing houses information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-skirt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:35:21.999615Z",
     "start_time": "2021-06-02T05:35:21.995141Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"https://people.sc.fsu.edu/~jburkardt/data/csv/homes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-method",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:35:23.163211Z",
     "start_time": "2021-06-02T05:35:22.683242Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-locator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:35:23.508847Z",
     "start_time": "2021-06-02T05:35:23.476008Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-salmon",
   "metadata": {},
   "source": [
    "Let's say that our task is to use the  columns `\"Living\", \"Rooms\", \"Beds\", \"Baths\", \"Age\", \"Acres\",\"Taxes\"` to predict if the `Sell` price is over or under our budget that is, for this example, 152k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-delhi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:46:57.397089Z",
     "start_time": "2021-06-02T05:46:57.374577Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Houses that were sold for more than 152k:\", (df.Sell > 152).sum())\n",
    "print(\"Houses that were sold for less than 152k:\", (df.Sell <= 152).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-appearance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:40:36.989297Z",
     "start_time": "2021-06-02T05:40:36.981228Z"
    }
   },
   "source": [
    "So we have a balanced dataset for this example. Instead of preprocessing our dataset *before* feeding the dataloader, let's do it inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-emphasis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:52:36.675390Z",
     "start_time": "2021-06-02T05:52:36.667671Z"
    }
   },
   "outputs": [],
   "source": [
    "class HouseDataset(Dataset):\n",
    "    def __init__(self, csv_file, budget=152):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df.columns = [x.replace('\"', '').replace(' ', '') for x in df.columns]\n",
    "        columns = [\"Living\", \"Rooms\", \"Beds\", \"Baths\", \"Age\", \"Acres\",\"Taxes\"]\n",
    "        self.X = df[columns].values # the .values takes the numpy array\n",
    "        self.y = (df.Sell.values <= budget).astype(\"int\")\n",
    "        self.n_samples = len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        # we want to be index like dataset[index]\n",
    "        # to get the index-th batch\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        # to retrieve the total samples by doing len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-jordan",
   "metadata": {},
   "source": [
    "Now we are ready to instantiate an object of the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-guyana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:52:38.078094Z",
     "start_time": "2021-06-02T05:52:37.567704Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = HouseDataset(dataset_path, budget=152)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-tunisia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:52:51.042524Z",
     "start_time": "2021-06-02T05:52:50.954338Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-investment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:52:53.705994Z",
     "start_time": "2021-06-02T05:52:53.698666Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-focus",
   "metadata": {},
   "source": [
    "Here we go! We have the batches, the shuffles and all we want in a similar manner of before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-sharp",
   "metadata": {},
   "source": [
    "### ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-switzerland",
   "metadata": {},
   "source": [
    "If you need to load an image dataset, it's more convenient to use the `ImageFolder` class from the `torchvision.datasets` module.\n",
    "\n",
    "To do so, you need to structure your data as follows:\n",
    "\n",
    "```\n",
    "root\n",
    "|_class1\n",
    "    |_xxx.png\n",
    "|_class2\n",
    "    |_xxx.png\n",
    "```\n",
    "\n",
    "that means that each class has its own directory.\n",
    "\n",
    "By giving this structure, the name of the class will be taken by the name of the folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-promotion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:18:18.117252Z",
     "start_time": "2021-06-02T06:18:18.087515Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "root_dir = 'datasets/my_dataset'\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                           [0.5, 0.5, 0.5])])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(root_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-complex",
   "metadata": {},
   "source": [
    "And then you just need to create the data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-freeware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:35:56.085748Z",
     "start_time": "2021-06-02T06:35:56.018127Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-ghost",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a dataset with three classes of images (choose the classes and download your own images. You don't need to train, so around 10 images per class will be enough).\n",
    "\n",
    "Then visualize the images with the help of the `imshow` helper function provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=False):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Run this to test your data loaders\n",
    "images, labels = next(iter(trainloader))\n",
    "imshow(images[0], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-metropolitan",
   "metadata": {},
   "source": [
    "### Image Dataset from paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-chester",
   "metadata": {},
   "source": [
    "Sometimes, you have given a bunch of paths and labels for your dataset, because it can be not convenient to move images around. \n",
    "\n",
    "For this reason, you can create something similar to what we have done at the beginning.\n",
    "\n",
    "Let's say we have a file called `train.csv` containing the columns `path` and `label`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "convertible-western",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:45:25.609577Z",
     "start_time": "2021-06-02T06:45:25.548882Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({\"path\": [\"my_dataset/image1.png\", \"my_dataset/image2.png\"], \"label\": [0, 1] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efficient-criminal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:45:29.569167Z",
     "start_time": "2021-06-02T06:45:29.550927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my_dataset/image1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my_dataset/image2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    path  label\n",
       "0  my_dataset/image1.png      0\n",
       "1  my_dataset/image2.png      1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "joint-raleigh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:46:11.305110Z",
     "start_time": "2021-06-02T06:46:11.217543Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-color",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:46:05.323220Z",
     "start_time": "2021-06-02T06:46:04.582143Z"
    }
   },
   "source": [
    "We can create a custom dataloader as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "velvet-shuttle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:51:27.202301Z",
     "start_time": "2021-06-02T06:51:27.195260Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.paths = df.path.values\n",
    "        self.labels = df.label.values\n",
    "    def __getitem__(self, index):\n",
    "        # we want to be index like dataset[index]\n",
    "        # to get the index-th batch\n",
    "        img = Image.open(self.paths[index]).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # to retrieve the total samples by doing len(dataset)\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-gibraltar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:53:50.432448Z",
     "start_time": "2021-06-02T06:53:50.415545Z"
    }
   },
   "source": [
    "### Optional/Advanced Exercise\n",
    "\n",
    "Create the csv file of the style defined above to load the dataset that you have created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-montgomery",
   "metadata": {},
   "source": [
    "You can customize even more! A nice article for it is:\n",
    "   https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-burner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
